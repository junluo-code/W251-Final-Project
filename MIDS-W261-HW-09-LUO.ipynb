{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW9\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Jun Luo*   \n",
    "__Class:__ MIDS w261 (Section *01*, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  *jun.luo*@iSchool.Berkeley.edu     \n",
    "__Unit 9:__   \n",
    "\n",
    "__Due Time:__ HW9 is due on FRIDAY 11/15/2016. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Intructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW  Problems](#3)   \n",
    "    1.0.  [HW9.0](#1.0)   \n",
    "    1.0.  [HW9.1](#1.1)   \n",
    "    1.2.  [HW9.2](#1.2)   \n",
    "    1.3.  [HW9.3](#1.3)    \n",
    "    1.4.  [HW9.4](#1.4)    \n",
    "    1.5.  [HW9.5](#1.5)    \n",
    "    1.5.  [HW9.6](#1.6)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\">\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #9\n",
    "\n",
    "Version 2016-11-01 \n",
    "\n",
    "### INSTRUCTIONS for SUBMISSIONS\n",
    "Please use the following form for HW submission:\n",
    "\n",
    "https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "HW9 can be completed locally on your computer for most part but will require a cluster of computers for the bigger wikipedia dataset.\n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "    \n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See async and live lectures for this week\n",
    "* Data-intensive text processing with MapReduce. San Rafael, CA: Morgan & Claypool Publishers. Chapter 5. \n",
    "\n",
    "\n",
    "\n",
    "<a name=\"3\">\n",
    "# HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\">HW 9 Dataset</h2>\n",
    "\n",
    "Note that all referenced files life in the enclosing directory. [Checkout the Data subdirectory on Dropbox](https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0) or the AWS S3 buckets (details contained each question). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW9.0  Short answer questions<a name=\"1.0\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "__ What is PageRank and what is it used for in the context of web search?__\n",
    "\n",
    "Per Wikipedia:\n",
    "\n",
    "PageRank is an algorithm used by Google Search to rank websites in their search engine results. It is a way of measuring the importance of website pages. According to Google:\n",
    "\n",
    "\"PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.\"\n",
    "\n",
    "<hr>\n",
    "\n",
    "__ What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to compute the Steady State Distibution? __\n",
    "\n",
    "To computer the steady state distribution under Perron Frobenius Theorem, the Markov chain process needs to have these two properties: irreducible and aperiodic. \n",
    "\n",
    "For web graph:\n",
    "\n",
    "Irreducible – There is a path from every node to every other node\n",
    "Aperiodic – The great common divisor of all cycle lengths is 1\n",
    "\n",
    "Teleportation and dangling node were introduced to satisfy the above well behaved graph properties so that we can leverage the Markov Chain calculation. At any time, the web surfer can randomly jump to a new node. For dangling nodes, their PageRank masses will be re-distributed to the whole graph so that they will not be lost.\n",
    "\n",
    "<hr>\n",
    "\n",
    "__ OPTIONAL: In topic-specific pagerank, how can we ensure that the irreducible property is satifsied? (HINT: see HW9.4) __\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW9.1  MRJob implementation of basic PageRank <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Write a basic MRJob implementation of the iterative PageRank algorithm that takes sparse adjacency lists as input (as explored in HW 7).\n",
    "\n",
    "Make sure that you implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iteration so that the output of each iteration is correctly normalized (sums to 1).\n",
    "\n",
    "\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page, chooses the next page to which it will move by clicking at random, with probability d,one of the hyperlinks in the current page. This probability is represented by a so-called *damping factor* d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer jumps to any web page in the network. If a page is a dangling end, meaning it has no outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform distribution and “teleports” to that page]\n",
    "\n",
    "\n",
    "As you build your code, use the test data:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "> Dropbox: https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck your work with the true result, displayed in the first image in the [Wikipedia article](https://en.wikipedia.org/wiki/PageRank)\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "<pre>\n",
    "\n",
    "A, 0.033\n",
    "B, 0.384\n",
    "C, 0.343\n",
    "D, 0.039\n",
    "E, 0.081\n",
    "F, 0.039\n",
    "G, 0.016\n",
    "H, 0.016\n",
    "I, 0.016\n",
    "J, 0.016\n",
    "K, 0.016\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.1 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankToy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankToy.py\n",
    "## Code goes here\n",
    "\n",
    "import ast\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "def init_graph(in_file, out_file):\n",
    "    nodes = []\n",
    "    with open(in_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            node = line.split('\\t')[0]\n",
    "            if node not in nodes:\n",
    "                nodes.append(node)\n",
    "                \n",
    "            linked_nodes = ast.literal_eval(line.split('\\t')[1])\n",
    "            for k,v in linked_nodes.items():\n",
    "                if k not in nodes:\n",
    "                    nodes.append(k)\n",
    "        \n",
    "    num_nodes = len(nodes)\n",
    "\n",
    "    with open(out_file, \"w\") as fout:\n",
    "        for n in sorted(nodes):\n",
    "            links = []\n",
    "            payload = \"{'score': \" + str(1.0/num_nodes) + \", \"\n",
    "            \n",
    "            with open(in_file,\"r\") as fin:\n",
    "                for line in fin:\n",
    "                    node = line.split('\\t')[0]\n",
    "                    linked_nodes = ast.literal_eval(line.split('\\t')[1])\n",
    "                    \n",
    "                    if n == node:\n",
    "                        for k,v in linked_nodes.items():\n",
    "                            links.append(k)\n",
    "            \n",
    "            payload += \"'links': \" + str(links) + \"}\"\n",
    "            \n",
    "            fout.write(n + '\\t' + payload +'\\n')\n",
    "            \n",
    "    return num_nodes\n",
    "\n",
    "class PageRankToy(MRJob):\n",
    "\n",
    "    INTERNAL_PROTOCOL = JSONProtocol\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        node = line.split('\\t')[0]\n",
    "        payload = ast.literal_eval(line.split('\\t')[1])\n",
    "        \n",
    "        score = payload['score']        \n",
    "        links = payload['links']\n",
    "        yield node, ('links', links) \n",
    "        \n",
    "        p = 0.0\n",
    "        if len(links) != 0:\n",
    "            p = float(score)/len(links)\n",
    "            for n in links:\n",
    "                yield n,('score', p)\n",
    "        else:\n",
    "            #store missing mass in counter\n",
    "            self.increment_counter('group', 'missing_mass', int(round(score,9)*10e+9))\n",
    "\n",
    "    def reducer(self, node, payload):\n",
    "        updated_score = 0\n",
    "        links = []\n",
    "        \n",
    "        for key, value in payload:\n",
    "            if key == 'links':\n",
    "                links = value\n",
    "            elif key == 'score':\n",
    "                updated_score += value\n",
    "\n",
    "        new_payload = \"{'score': \" + str(updated_score) + \", \"\n",
    "        new_payload += \"'links': \" + str(links) + \"}\"\n",
    "\n",
    "        yield node, new_payload\n",
    "        \n",
    "        \n",
    "    def steps(self):\n",
    "        return ([MRStep(\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer)\n",
    "                ]) \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    PageRankToy.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankToy_Driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankToy_Driver.py\n",
    "\n",
    "## Drivers & Runners\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "from PageRankToy import PageRankToy, init_graph\n",
    "\n",
    "input_graph_file = \"PageRank-test.txt\"\n",
    "output_graph_file = \"graph.txt\"\n",
    "\n",
    "N = init_graph(input_graph_file, output_graph_file)\n",
    "mr_job = PageRankToy(args=[output_graph_file])\n",
    "\n",
    "i = 0\n",
    "\n",
    "#get input parameters\n",
    "d = float(sys.argv[1]) #damping factor\n",
    "max_iterations = int(sys.argv[2])\n",
    "debug = sys.argv[3]\n",
    "\n",
    "a = 1-d #teleportation factor\n",
    "\n",
    "while(True):\n",
    "\n",
    "    p_sum = 0.0\n",
    "    payload = {}\n",
    "    \n",
    "    if debug==\"true\": print \"iteration %d:\"%(i+1)\n",
    "    if debug==\"true\": print \"---------------------------------------------\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        missing_mass = float(runner.counters()[0]['group']['missing_mass'])/10e+9\n",
    "        \n",
    "        \n",
    "        # stream_output: get access of the output \n",
    "        with open(output_graph_file,\"w\") as f:\n",
    "            for line in runner.stream_output():\n",
    "                key,value =  mr_job.parse_output_line(line)\n",
    "                \n",
    "                payload = ast.literal_eval(value)\n",
    "                score = float(payload['score'])\n",
    "                new_score =  a*(1.0/N) + d*(missing_mass/N + score)\n",
    "                \n",
    "                p_sum += new_score\n",
    "                #print key, value, score, new_score\n",
    "                \n",
    "                new_value = \"{'score': \" + str(new_score) + \", \"\n",
    "                new_value += \"'links': \" + str(payload['links']) + \"}\"\n",
    "                \n",
    "                if debug==\"true\": print key, new_value\n",
    "                f.write(key+'\\t'+new_value+'\\n')\n",
    "\n",
    "    missing_mass = float(runner.counters()[0]['group']['missing_mass'])/10e+9\n",
    "    if debug==\"true\": print \"\\nmissing mass: %10.3f\"%(missing_mass)\n",
    "    if debug==\"true\": print \"probability distribution: %10.3f\\n\\n\"%(p_sum)\n",
    "    \n",
    "    i = i + 1\n",
    "    if(i >= max_iterations):\n",
    "        break\n",
    "\n",
    "print \"======================================================\"\n",
    "print \"PageRank after %d iterations with damping factor=%4.2f\"%(max_iterations, d)\n",
    "print \"======================================================\"\n",
    "with open(output_graph_file,\"r\") as f:\n",
    "    for line in f:\n",
    "        node = line.split('\\t')[0]\n",
    "        value = ast.literal_eval(line.split('\\t')[1])\n",
    "        for k,v in value.items():\n",
    "            if k==\"score\":\n",
    "                print\"%s%8.3f\"%(node,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "PageRank after 30 iterations with damping factor=0.85\n",
      "======================================================\n",
      "A   0.033\n",
      "B   0.384\n",
      "C   0.344\n",
      "D   0.039\n",
      "E   0.081\n",
      "F   0.039\n",
      "G   0.016\n",
      "H   0.016\n",
      "I   0.016\n",
      "J   0.016\n",
      "K   0.016\n",
      "\n",
      "Graph data:\n",
      "A\t{'score': 0.0327814940127, 'links': []}\n",
      "B\t{'score': 0.383596812817, 'links': ['C']}\n",
      "C\t{'score': 0.343714416648, 'links': ['B']}\n",
      "D\t{'score': 0.0390870930929, 'links': ['A', 'B']}\n",
      "E\t{'score': 0.0808856947411, 'links': ['B', 'D', 'F']}\n",
      "F\t{'score': 0.0390870930929, 'links': ['B', 'E']}\n",
      "G\t{'score': 0.0161694791591, 'links': ['B', 'E']}\n",
      "H\t{'score': 0.0161694791591, 'links': ['B', 'E']}\n",
      "I\t{'score': 0.0161694791591, 'links': ['B', 'E']}\n",
      "J\t{'score': 0.0161694791591, 'links': ['E']}\n",
      "K\t{'score': 0.0161694791591, 'links': ['E']}\n"
     ]
    }
   ],
   "source": [
    "## Run Scripts, S3 Sync\n",
    "## Usage: python driver.py damping_factor iterations debug_flag\n",
    "!python PageRankToy_Driver.py 0.85 30 false\n",
    "!echo\n",
    "!echo \"Graph data:\"\n",
    "!cat graph.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.1 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW9.2: Exploring PageRank teleportation and network plots <a name=\"1.2\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "* In order to overcome  problems such as disconnected components, the damping factor (a typical value for d is 0.85) can be varied. \n",
    "* Using the graph in HW1, plot the test graph (using networkx, https://networkx.github.io/) for several values of the damping parameter alpha, so that each nodes radius is proportional to its PageRank score. \n",
    "* In particular you should do this for the following damping factors: [0,0.25,0.5,0.75, 0.85, 1]. \n",
    "* Note your plots should look like the following: https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.2 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Code goes here\n",
    "import networkx as nx\n",
    "import ast\n",
    "import subprocess\n",
    "\n",
    "def get_Color(score):\n",
    "    \n",
    "    colors = ['#FFFFFF','#E2FFE2','#C6FFC6','#AAFFAA','#8DFF8D','#71FF71','#55FF55','#38FF38','#1CFF1C','#00FF00']\n",
    "    return colors[int(score*10)]\n",
    "\n",
    "def draw_Graph(title):\n",
    "    nodes, sizes, edges, colors,labels = [], [], [], [], {}\n",
    "    default_node_size = 12000\n",
    "    \n",
    "    with open(\"graph.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            node = line.split('\\t')[0]\n",
    "            nodes.append(node)\n",
    "            \n",
    "            value = ast.literal_eval(line.split('\\t')[1])\n",
    "            for k,v in value.items():\n",
    "                if k==\"score\":\n",
    "                    score = round(float(v),3)\n",
    "                    labels[node] = node + '\\n' + str(score)\n",
    "                    sizes.append(default_node_size*score)\n",
    "                    colors.append(get_Color(score))\n",
    "                elif k==\"links\":\n",
    "                    for n in v:\n",
    "                        edges.append((node,n))\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    ax = plt.gca()\n",
    "    ax.autoscale_view('tight')\n",
    "    \n",
    "    \n",
    "    G=nx.DiGraph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "    pos=nx.shell_layout(G)\n",
    "\n",
    "    nx.draw_networkx_nodes(G,pos,\n",
    "                           nodelist=nodes,\n",
    "                           node_color=colors,\n",
    "                           node_size=sizes, alpha=0.5)\n",
    "\n",
    "    nx.draw_networkx_edges(G,pos,\n",
    "                           edgelist=edges,\n",
    "                           width=1,alpha=1,edge_color='grey')\n",
    "\n",
    "    nx.draw_networkx_labels(G,pos,labels,font_size=10)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "damping_factors = [0,0.25,0.5,0.75, 0.85, 1]\n",
    "for d in damping_factors:\n",
    "    \n",
    "    cmd = \"python PageRankToy_Driver.py \" + str(d) + \" 30 false\"\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    title = \"Damping Factor = \" + str(round(d,2))\n",
    "    draw_Graph(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.2 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW9.3: Applying PageRank to the Wikipedia hyperlinks network <a name=\"1.3\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "* Run your PageRank implementation on the Wikipedia dataset for 5 iterations, and display the top 100 ranked nodes (with alpha = 0.85).\n",
    "* Run your PageRank implementation on the Wikipedia dataset for 10 iterations, and display the top 100 ranked nodes (with teleportation factor of 0.15).\n",
    "* Have the top 100 ranked pages changed? Comment on your findings. \n",
    "* Plot the pagerank values for the top 100 pages resulting from the 5 iterations run. Then plot the pagerank values for the same 100 pages that resulted from the 10 iterations run.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.3 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 style=\"color:darkgreen\">  HW 9.3 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW9.4: Topic-specific PageRank implementation using MRJob <a name=\"1.4\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Modify your PageRank implementation to produce a topic specific PageRank implementation, as described in:\n",
    "\n",
    "http://www-cs-students.stanford.edu/~taherh/papers/topic-sensitive-pagerank.pdf\n",
    "\n",
    "Note in this article that there is a special caveat to ensure that the transition matrix is irreducible.   \n",
    "This caveat lies in footnote 3 on page 3:\n",
    "```\n",
    "\tA minor caveat: to ensure that M is irreducible when p\n",
    "\tcontains any 0 entries, nodes not reachable from nonzero\n",
    "\tnodes in p should be removed. In practice this is not problematic.\n",
    "```\n",
    "and must be adhered to for convergence to be guaranteed.   \n",
    "\n",
    "Run topic specific PageRank on the following randomly generated network of 100 nodes:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/randNet.txt (also available on Dropbox)\n",
    "\n",
    "which are organized into ten topics, as described in the file:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/randNet_topics.txt  (also available on Dropbox)\n",
    "\n",
    "Since there are 10 topics, your result should be 11 PageRank vectors (one for the vanilla PageRank implementation in 9.1, and one for each topic with the topic specific implementation). Print out the top ten ranking nodes and their topics for each of the 11 versions, and comment on your result. Assume a teleportation factor of 0.15 in all your analyses.\n",
    "\n",
    "One final and important comment here:  please consider the requirements for irreducibility with topic-specific PageRank. In particular, the literature ensures irreducibility by requiring that nodes not reachable from in-topic nodes be removed from the network.\n",
    "\n",
    "This is not a small task, especially as it it must be performed separately for each of the (10) topics.\n",
    "\n",
    "So, instead of using this method for irreducibility, please comment on why the literature's method is difficult to implement, and what what extra computation it will require.   \n",
    "\n",
    "Then for your code, please use the alternative, non-uniform damping vector:\n",
    "\n",
    "```\n",
    "vji = beta*(1/|Tj|); if node i lies in topic Tj\n",
    "\n",
    "vji = (1-beta)*(1/(N - |Tj|)); if node i lies outside of topic Tj\n",
    "```\n",
    "for beta in (0,1) close to 1. \n",
    "\n",
    "With this approach, you will not have to delete any nodes. If beta > 0.5, PageRank is topic-sensitive, and if beta < 0.5, the PageRank is anti-topic-sensitive. For any value of beta irreducibility should hold, so please try beta=0.99, and perhaps some other values locally, on the smaller networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.4 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.4 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div class='jumbotron'><h3 style='color:darkblue'>---------  OPTIONAL QUESTIONS SECTION --------</h3></div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW9.5: (OPTIONAL) Applying topic-specific PageRank to Wikipedia <a name=\"1.5\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "Here you will apply your topic-specific PageRank implementation to Wikipedia, defining topics (very arbitrarily) for each page by the length (number of characters) of the name of the article mod 10, so that there are 10 topics. \n",
    "\n",
    "* Once again, print out the top ten ranking nodes and their topics for each of the 11 versions, and comment on your result. Assume a teleportation factor of 0.15 in all your analyses. Run for 10 iterations.\n",
    "* Plot the pagerank values for the top 100 pages resulting from the 5 iterations run in HW 9.3. \n",
    "* Then plot the pagerank values for the same 100 pages that result from the topic specific pagerank after 10 iterations run. \n",
    "* Comment on your findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.5 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.5 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW9.6:  (OPTIONAL) TextRank <a name=\"1.6\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "* What is TextRank? Describe the main steps in the algorithm. Why does TextRank work?\n",
    "* Implement TextRank in MrJob for keyword phrases (not just unigrams) extraction using co-occurrence based similarity measure with with sizes of N = 2 and 3. And evaluate your code using the following example using precision, recall, and FBeta (Beta=1):\n",
    "```\n",
    "\"Compatibility of systems of linear constraints over the set of natural numbers\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict \n",
    "inequations, and nonstrict inequations are considered. Upper bounds for\n",
    "components of a minimal set of solutions and algorithms of construction of \n",
    "minimal generating sets of solutions for all types of systems are given. \n",
    "These criteria and the corresponding algorithms for constructing a minimal \n",
    "supporting set of solutions can be used in solving all the considered types of \n",
    "systems and systems of mixed types.\" \n",
    "```\n",
    "* The extracted keywords should in the following set:\n",
    "```\n",
    "linear constraints, linear diophantine equations, natural numbers, non-strict inequations, strict inequations, upper bounds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.6 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.6 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div class='jumbotron'><h2 style='color:green'>-------  END OF HWK 9 --------</h2></div></center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
